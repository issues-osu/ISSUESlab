<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Hugo Blox Builder 5.9.7"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&family=Merriweather&family=Roboto+Mono&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&family=Merriweather&family=Roboto+Mono&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/issues/css/vendor-bundle.min.26c458e6907dc03073573976b7f4044e.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css integrity="sha512-IW0nhlW5MgNydsXJO40En2EoCkTTjZhI3yuODrZIc8cQ4h1XcF53PsqDHa09NqnkXuIe0Oiyyj171BqZFwISBw==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/issues/css/wowchemy.bd346f2d55b4b82e0e60f371ed694d36.css><link rel=stylesheet href=/issues/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/issues/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><script async src="https://www.googletagmanager.com/gtag/js?id=UA-143417043-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}function trackOutboundLink(e,t){gtag("event","click",{event_category:"outbound",event_label:e,transport_type:"beacon",event_callback:function(){t!=="_blank"&&(document.location=e)}}),console.debug("Outbound link clicked: "+e)}function onClickCallback(e){if(e.target.tagName!=="A"||e.target.host===window.location.host)return;trackOutboundLink(e.target,e.target.getAttribute("target"))}gtag("js",new Date),gtag("config","UA-143417043-1",{}),gtag("set",{cookie_flags:"SameSite=None;Secure"}),document.addEventListener("click",onClickCallback,!1)</script><meta name=author content="Assistant Professor, Social Work & Public Health"><meta name=description content="Open stuff"><link rel=alternate hreflang=en-us href=https://issues-osu/issues/data-code/><link rel=canonical href=https://issues-osu/issues/data-code/><link rel=manifest href=/issues/manifest.webmanifest><link rel=icon type=image/png href=/issues/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/issues/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@data4socialjustice.bsky.social"><meta property="twitter:creator" content="@data4socialjustice.bsky.social"><meta property="twitter:image" content="https://issues-osu/issues/media/logo_hu2fedd529b9d9d7008e9f6968cd645fa1_199187_300x300_fit_lanczos_3.png"><meta property="og:type" content="website"><meta property="og:site_name" content="ISSUES | The Ohio State University"><meta property="og:url" content="https://issues-osu/issues/data-code/"><meta property="og:title" content="Data and code | ISSUES | The Ohio State University"><meta property="og:description" content="Open stuff"><meta property="og:image" content="https://issues-osu/issues/media/logo_hu2fedd529b9d9d7008e9f6968cd645fa1_199187_300x300_fit_lanczos_3.png"><meta property="og:locale" content="en-us"><title>Data and code | ISSUES | The Ohio State University</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=9f90719502b0413cefb3a0a3367c7355><script src=/issues/js/wowchemy-init.min.4fef3e534144e9903491f0cc6527eccd.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/issues/><img src=/issues/media/logo_hu2fedd529b9d9d7008e9f6968cd645fa1_199187_0x70_resize_lanczos_3.png alt="ISSUES | The Ohio State University"></a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/issues/><img src=/issues/media/logo_hu2fedd529b9d9d7008e9f6968cd645fa1_199187_0x70_resize_lanczos_3.png alt="ISSUES | The Ohio State University"></a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>About</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/issues/author><span>Team</span></a>
<a class=dropdown-item href=/issues/news><span>Media</span></a></div></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Research Areas</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/issues/research/reproductive-justice><span>Maternal & Child Health</span></a>
<a class=dropdown-item href=/issues/research/education><span>Education</span></a>
<a class=dropdown-item href=/issues/research/aces><span>ACEs</span></a>
<a class=dropdown-item href=/issues/research/overdose><span>Drug Overdose</span></a>
<a class=dropdown-item href=/issues/research/violence><span>Interpersonal Violence</span></a></div></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Law & Policy</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/issues/legal><span>Legal Projects</span></a>
<a class=dropdown-item href=/issues/policy-briefs><span>Policy Briefs</span></a></div></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Blog</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=https://bigdataforsocialjustice.medium.com/><span>BD4SJ on Medium</span></a></div></li><li class=nav-item><a class=nav-link href=/issues/publication><span>Publications</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Resources</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/issues/resources/datasets><span>Datasets</span></a>
<a class=dropdown-item href=/issues/courses><span>Tutorials</span></a>
<a class=dropdown-item href=/issues/resources/resources><span>Learning</span></a></div></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Join Us</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/issues/opportunities/vacancies><span>Openings</span></a>
<a class=dropdown-item href=/issues/opportunities/application-guide><span>How to Apply</span></a>
<a class=dropdown-item href=/issues/opportunities/student-projects><span>Undergraduate Students</span></a></div></li><li class=nav-item><a class=nav-link href=/issues/#contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li></ul></div></nav></header></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>Data and code</h1><p class=page-subtitle>Open resources from our group</p><div class=article-metadata></div></div><div class=article-container><div class=article-style><p>We publish most of our code and data openly.
We do that mostly through <a href=https://github.com/ualsg target=_blank rel=noopener>our Github account</a>.</p><p>Please see below for a list of resources and projects, especially lab-grown datasets, which we are happy to share.
On this page, we also include similar <a href=#with-our-friendly-collaborators>outputs led by collaborators</a>, in which we were involved.</p><p>On a related note, you may be interested in <a href=https://doi.org/10.1016/j.compenvurbsys.2022.101825 target=_blank rel=noopener>our comprehensive survey on open-source software</a>, published as a review paper in <em>Computers, Environment and Urban Systems</em>.</p><p>In general, all resources are released under a liberal licence, enabling you unrestricted use as long as you attribute them.
If you use the code and/or the data for presentations and publications, we kindly ask you to cite the related paper(s) and credit our work.
We have provided guidelines to do so in each resource.</p><p>The usual caveat: while a great deal of effort has been put into each project, they are not free of errors, and we cannot be held responsible for the use of code and/or data and any issues that may arise.</p><p>Feel free to contact us for more information, report bugs and errors, or simply to inform us what are you using the data for.
We would be pleased to learn how others are using our work.
If you are interested in collaborating with us, please get in touch with the lead developer of each resource listed below.</p><h1 id=in-progress>In progress</h1><h2 id=spatial-accessibility>Spatial Accessibility</h2><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt srcset="/issues/data-code/global-streetscapes_hu573ff7f36dc20486ea88cd3b0d8461d2_52301_f56e8415412b855fbc558cb4345ad8a7.webp 400w,
/issues/data-code/global-streetscapes_hu573ff7f36dc20486ea88cd3b0d8461d2_52301_b9fe0bbe106d1cc01f9f1268c1077952.webp 760w,
/issues/data-code/global-streetscapes_hu573ff7f36dc20486ea88cd3b0d8461d2_52301_1200x1200_fit_q80_h2_lanczos.webp 1200w" src=/issues/data-code/global-streetscapes_hu573ff7f36dc20486ea88cd3b0d8461d2_52301_f56e8415412b855fbc558cb4345ad8a7.webp width=760 height=543 loading=lazy data-zoomable></div></div></figure></p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Short description:</td><td>Spatial Accessibility to Abortion Facilities in Ohio, Kentucky, and West Virginia.</td></tr><tr><td>Lead developer:</td><td><a href=/issues/author/barboza/>Barboza</a></td></tr><tr><td>Further reading:</td><td>Please read the <a href=https://doi.org/10.1016/j.isprsjprs.2024.06.023 target=_blank rel=noopener>paper</a> published in the ISPRS Journal of Photogrammetry and Remote Sensing</td></tr><tr><td>Download:</td><td><a href=https://github.com/ualsg/global-streetscapes target=_blank rel=noopener><i class="fab fa-github"></i> Github repo</a></td></tr><tr><td>Main data source(s):</td><td>Mapillary, KartaView, OpenStreetMap, GADM, &mldr; and manual labelling</td></tr><tr><td>Coverage:</td><td>688 cities around the world</td></tr><tr><td>Citation:</td><td><details class=spoiler id=spoiler-1><summary>Click to view the BibTeX entry</summary><p>@article{2024_global_streetscapes,
author = {Hou, Yujun and Quintana, Matias and Khomiakov, Maxim and Yap, Winston and Ouyang, Jiani and Ito, Koichi and Wang, Zeyu and Zhao, Tianhong and Biljecki, Filip},
doi = {10.1016/j.isprsjprs.2024.06.023},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
pages = {216-238},
title = {Global Streetscapes &ndash; A comprehensive dataset of 10 million street-level images across 688 cities for urban science and analytics},
volume = {215},
year = {2024}
}</p></details></td></tr></tbody></table><p>Check out this <a href=/lab/markdown/spatialaccess.html>Access our tutorial on spatial accessibility here</a> for more details.</p><h2 id=machine-learning>Machine Learning</h2><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt srcset="/issues/data-code/urbanity_hue8b563648e405456e2de25399aefc3d5_321271_d56f0a772e1f17fff895cbc56e2aeedd.webp 400w,
/issues/data-code/urbanity_hue8b563648e405456e2de25399aefc3d5_321271_76243899cb0c71d83314a57dc95cdde7.webp 760w,
/issues/data-code/urbanity_hue8b563648e405456e2de25399aefc3d5_321271_1200x1200_fit_q80_h2_lanczos_3.webp 1200w" src=/issues/data-code/urbanity_hue8b563648e405456e2de25399aefc3d5_321271_d56f0a772e1f17fff895cbc56e2aeedd.webp width=760 height=272 loading=lazy data-zoomable></div></div></figure></p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Short description:</td><td>A network-based python package to understand and model urban complexity</td></tr><tr><td>Lead developer:</td><td>winston</td></tr><tr><td>Further reading:</td><td>Please read the <a href=https://doi.org/10.1038/s42949-023-00125-w target=_blank rel=noopener>paper</a> published in npj Urban Sustainability</td></tr><tr><td>Code:</td><td><a href=https://github.com/winstonyym/urbanity target=_blank rel=noopener><i class="fab fa-github"></i> Github repo</a></td></tr><tr><td>Data:</td><td><a href=https://doi.org/10.6084/m9.figshare.22124219 target=_blank rel=noopener><i class="fa-solid fa-download"></i> Figshare</a></td></tr><tr><td>Main data source(s):</td><td>OpenStreetMap, Mapillary, etc.</td></tr><tr><td>Citation:</td><td><details class=spoiler id=spoiler-3><summary>Click to view the BibTeX entry</summary><p>@article{2023_npjus_urbanity,
author = {Yap, Winston and Stouffs, Rudi and Biljecki, Filip},
doi = {10.1038/s42949-023-00125-w},
journal = {npj Urban Sustainability},
title = {{Urbanity: automated modelling and analysis of multidimensional networks in cities}},
volume = {3},
issue = {45},
year = {2023}
}</p></details></td></tr></tbody></table><h2 id=predicting-building-characteristics-using-graph-neural-networks-and-street-level-contexts>Predicting building characteristics using graph neural networks and street-level contexts</h2><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt srcset="/issues/data-code/building-gnn_hu38c1f7b760a74819c095a7fce354e6d1_271901_39800178d1baa8ffb725105eb00a4995.webp 400w,
/issues/data-code/building-gnn_hu38c1f7b760a74819c095a7fce354e6d1_271901_7ce4c8d9ce3eefdbb446f6762e61e8dd.webp 760w,
/issues/data-code/building-gnn_hu38c1f7b760a74819c095a7fce354e6d1_271901_1200x1200_fit_q80_h2_lanczos.webp 1200w" src=/issues/data-code/building-gnn_hu38c1f7b760a74819c095a7fce354e6d1_271901_39800178d1baa8ffb725105eb00a4995.webp width=760 height=458 loading=lazy data-zoomable></div></div></figure></p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Short description:</td><td>This repo is for our paper &ldquo;Predicting building characteristics using graph neural networks and street-level contexts&rdquo;</td></tr><tr><td>Lead developer:</td><td>binyu</td></tr><tr><td>Further reading:</td><td>Please read the <a href=https://doi.org/10.1016/j.compenvurbsys.2024.102129 target=_blank rel=noopener>paper</a> published in CEUS</td></tr><tr><td>Code:</td><td><a href=https://github.com/binyulei/gnn-building-characteristics-prediction target=_blank rel=noopener><i class="fab fa-github"></i> Github repo</a></td></tr><tr><td>Citation:</td><td><details class=spoiler id=spoiler-5><summary>Click to view the BibTeX entry</summary><p>@article{2024_ceus_gnn_building,
author = {Lei, Binyu and Liu, Pengyuan and Milojevic-Dupont, Nikola and Biljecki, Filip},
doi = {10.1016/j.compenvurbsys.2024.102129},
journal = {Computers, Environment and Urban Systems},
pages = {102129},
title = {Predicting building characteristics at urban scale using graph neural networks and street-level context},
volume = {111},
year = {2024}
}</p></details></td></tr></tbody></table><h2 id=explainable-spatially-explicit-geospatial-artificial-intelligence-in-urban-analytics>Explainable spatially explicit geospatial artificial intelligence in urban analytics</h2><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt srcset="/issues/data-code/geoxai_huf8e25c2e130ea8a9373271103820a6d0_2533655_7301f037803d4acf021bc84e707cba1e.webp 400w,
/issues/data-code/geoxai_huf8e25c2e130ea8a9373271103820a6d0_2533655_27358714d658792bb80f1f0cbf258790.webp 760w,
/issues/data-code/geoxai_huf8e25c2e130ea8a9373271103820a6d0_2533655_1200x1200_fit_q80_h2_lanczos_3.webp 1200w" src=/issues/data-code/geoxai_huf8e25c2e130ea8a9373271103820a6d0_2533655_7301f037803d4acf021bc84e707cba1e.webp width=760 height=272 loading=lazy data-zoomable></div></div></figure></p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Short description:</td><td>This repo is for our paper &ldquo;Explainable spatially explicit geospatial artificial intelligence in urban analytics&rdquo;</td></tr><tr><td>Lead developer:</td><td>pengyuan</td></tr><tr><td>Further reading:</td><td>Please read the <a href=https://doi.org/10.1177/23998083231204689 target=_blank rel=noopener>paper</a> published in EPB</td></tr><tr><td>Code:</td><td><a href=https://github.com/PengyuanLiu1993/XAI-Urban-Analytics target=_blank rel=noopener><i class="fab fa-github"></i> Github repo</a></td></tr><tr><td>Citation:</td><td><details class=spoiler id=spoiler-7><summary>Click to view the BibTeX entry</summary><p>@article{2024_epb_xai,
author = {Liu, Pengyuan and Yan, Zhang and Biljecki, Filip},
doi = {10.1177/23998083231204689},
journal = {Environment and Planning B: Urban Analytics and City Science},
pages = {1104&ndash;1123},
title = {{Explainable spatially explicit geospatial artificial intelligence in urban analytics}},
volume = {51},
issue = {5},
year = {2024}
}</p></details></td></tr></tbody></table><h2 id=computer-vision-and-graph-models-to-predict-outdoor-comfort>Computer Vision and Graph Models to Predict Outdoor Comfort</h2><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt srcset="/issues/data-code/sidewalks_huf67ab1f6abd1cc02d1e621a47a264e34_679549_63bdbd9037044a65f995368c4303a1a1.webp 400w,
/issues/data-code/sidewalks_huf67ab1f6abd1cc02d1e621a47a264e34_679549_950a4b88d5a41c30b0eaba8a7463e6ee.webp 760w,
/issues/data-code/sidewalks_huf67ab1f6abd1cc02d1e621a47a264e34_679549_1200x1200_fit_q80_h2_lanczos.webp 1200w" src=/issues/data-code/sidewalks_huf67ab1f6abd1cc02d1e621a47a264e34_679549_63bdbd9037044a65f995368c4303a1a1.webp width=760 height=408 loading=lazy data-zoomable></div></div></figure></p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Short description:</td><td>This repo is for our paper &ldquo;Towards Human-centric Digital Twins: Leveraging Computer Vision and Graph Models to Predict Outdoor Comfort&rdquo;</td></tr><tr><td>Lead developer:</td><td>pengyuan</td></tr><tr><td>Further reading:</td><td>Please read the <a href=https://doi.org/10.1016/j.scs.2023.104480 target=_blank rel=noopener>paper</a> published in SCS</td></tr><tr><td>Code:</td><td><a href=https://github.com/PengyuanLiu1993/GSL-sidewalk-comfort target=_blank rel=noopener><i class="fab fa-github"></i> Github repo</a></td></tr><tr><td>Main data source(s):</td><td>Own field survey</td></tr><tr><td>Citation:</td><td><details class=spoiler id=spoiler-9><summary>Click to view the BibTeX entry</summary><p>@article{2023_scs_human_dt,
author = {Liu, Pengyuan and Zhao, Tianhong and Luo, Junjie and Lei, Binyu and Frei, Mario and Miller, Clayton and Biljecki, Filip},
doi = {10.1016/j.scs.2023.104480},
journal = {Sustainable Cities and Society},
pages = {104480},
title = {{Towards Human-centric Digital Twins: Leveraging Computer Vision and Graph Models to Predict Outdoor Comfort}},
volume = {93},
year = {2023}
}</p></details></td></tr></tbody></table><h2 id=instantcity---synthesising-morphologically-accurate-geospatial-data-for-urban-form-analysis-transfer-and-quality-control>InstantCITY - Synthesising morphologically accurate geospatial data for urban form analysis, transfer, and quality control</h2><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt srcset="/issues/data-code/instantcity_hu64cb68be8fe86bfe86580fdae95956ca_393911_c65326c986539162f7f04861a5b935ab.webp 400w,
/issues/data-code/instantcity_hu64cb68be8fe86bfe86580fdae95956ca_393911_50666f944c25bfe4bf464e6302ff0c93.webp 760w,
/issues/data-code/instantcity_hu64cb68be8fe86bfe86580fdae95956ca_393911_1200x1200_fit_q80_h2_lanczos_3.webp 1200w" src=/issues/data-code/instantcity_hu64cb68be8fe86bfe86580fdae95956ca_393911_c65326c986539162f7f04861a5b935ab.webp width=760 height=272 loading=lazy data-zoomable></div></div></figure></p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Short description:</td><td>Generating vectorised building footprint data from street networks using Generative Adversarial Networks</td></tr><tr><td>Lead developer:</td><td>abraham</td></tr><tr><td>Further reading:</td><td>Please read the <a href=https://doi.org/10.1016/j.isprsjprs.2022.11.005 target=_blank rel=noopener>paper</a> published in IJPRS</td></tr><tr><td>Code:</td><td><a href=https://github.com/ualsg/InstantCity target=_blank rel=noopener><i class="fab fa-github"></i> Github repo</a></td></tr><tr><td>Main data source(s):</td><td>OpenStreetMap</td></tr><tr><td>Citation:</td><td><details class=spoiler id=spoiler-11><summary>Click to view the BibTeX entry</summary><p>@article{2023_ijprs_instantcity,
author = {Wu, Abraham Noah and Biljecki, Filip},
doi = {10.1016/j.isprsjprs.2022.11.005},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
pages = {90-104},
title = {InstantCITY: Synthesising morphologically accurate geospatial data for urban form analysis, transfer, and quality control},
volume = {195},
year = {2023}
}</p></details></td></tr></tbody></table><h2 id=water-view-imagery>Water View Imagery</h2><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt srcset="/issues/data-code/water-view-imagery_hua77cdf5ffe59f9c0c626f4798139b432_625114_32c129ae34a28324df968fd384f7dadf.webp 400w,
/issues/data-code/water-view-imagery_hua77cdf5ffe59f9c0c626f4798139b432_625114_641940e9c4a4452458a5ebc9c8f04da4.webp 760w,
/issues/data-code/water-view-imagery_hua77cdf5ffe59f9c0c626f4798139b432_625114_1200x1200_fit_q80_h2_lanczos_3.webp 1200w" src=/issues/data-code/water-view-imagery_hua77cdf5ffe59f9c0c626f4798139b432_625114_32c129ae34a28324df968fd384f7dadf.webp width=760 height=222 loading=lazy data-zoomable></div></div></figure></p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Short description:</td><td>On-water perspective imagery dataset for semantic segmentation of waterscapes.</td></tr><tr><td>Lead developer:</td><td>junjie</td></tr><tr><td>Further reading:</td><td>Please read the <a href=https://doi.org/10.1016/j.ecolind.2022.109615 target=_blank rel=noopener>paper</a> published in Ecological Indicators</td></tr><tr><td>Download:</td><td><a href=https://github.com/ualsg/Water-View-Imagery-dataset target=_blank rel=noopener><i class="fab fa-github"></i> Github repo</a></td></tr><tr><td>Main data source(s):</td><td>Mapillary and manual labelling</td></tr><tr><td>Coverage:</td><td>Eight cities: Amsterdam, Bangkok, Chicago, Istanbul, Japan, London, Paris, and Venice</td></tr><tr><td>Citation:</td><td><details class=spoiler id=spoiler-13><summary>Click to view the BibTeX entry</summary><p>@article{2022_ei_water_view_imagery,
author = {Luo, Junjie and Zhao, Tianhong and Cao, Lei and Biljecki, Filip},
doi = {10.1016/j.ecolind.2022.109615},
journal = {Ecological Indicators},
pages = {109615},
title = {Water View Imagery: Perception and evaluation of urban waterscapes worldwide},
volume = {145},
year = {2022}
}</p></details></td></tr></tbody></table><h2 id=global-urban-road-network-patterns>Global Urban Road Network Patterns</h2><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt srcset="/issues/data-code/gurnp_hub59b648db36c4f87102bc4ef6bb25fe1_2564340_7c5bd210328b565cfe524e516eb045e2.webp 400w,
/issues/data-code/gurnp_hub59b648db36c4f87102bc4ef6bb25fe1_2564340_7fd9fd16a464ac9659ed874db7118377.webp 760w,
/issues/data-code/gurnp_hub59b648db36c4f87102bc4ef6bb25fe1_2564340_1200x1200_fit_q80_h2_lanczos_3.webp 1200w" src=/issues/data-code/gurnp_hub59b648db36c4f87102bc4ef6bb25fe1_2564340_7c5bd210328b565cfe524e516eb045e2.webp width=760 height=248 loading=lazy data-zoomable></div></div></figure></p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Short description:</td><td>Deep learning-based analysis of the urban morphology around the world</td></tr><tr><td>Lead developer:</td><td>wangyang</td></tr><tr><td>Further reading:</td><td>Please read the <a href=https://doi.org/10.1016/j.landurbplan.2023.104901 target=_blank rel=noopener>paper</a> published in Landscape and Urban Planning</td></tr><tr><td>Code:</td><td><a href=https://github.com/ualsg/Global-road-network-patterns target=_blank rel=noopener><i class="fab fa-github"></i> Github repo</a></td></tr><tr><td>Main data source(s):</td><td>OpenStreetMap</td></tr><tr><td>Coverage:</td><td>144 cities</td></tr><tr><td>Citation:</td><td><details class=spoiler id=spoiler-15><summary>Click to view the BibTeX entry</summary><p>@article{2024_land_urn,
author = {Chen, Wangyang and Huang, Huiming and Liao, Shunyi and Gao, Feng and Biljecki, Filip},
doi = {10.1016/j.landurbplan.2023.104901},
journal = {Landscape and Urban Planning},
title = {{Global urban road network patterns: Unveiling multiscale planning paradigms of 144 cities with a novel deep learning approach}},
volume = {241},
pages = {104901},
year = {2024}
}</p></details></td></tr></tbody></table><h2 id=sviqc----street-view-imagery-quality-checker>SVIQC &ndash; Street View Imagery Quality Checker</h2><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt srcset="/issues/data-code/kowloon-grid_hu0ba43ba588d0d4b529a9453da4d2baae_307717_f0b163cf01b945a99c59287592d74ecc.webp 400w,
/issues/data-code/kowloon-grid_hu0ba43ba588d0d4b529a9453da4d2baae_307717_74c897ec2c125df39f29ceb2b68a652b.webp 760w,
/issues/data-code/kowloon-grid_hu0ba43ba588d0d4b529a9453da4d2baae_307717_1200x1200_fit_q80_h2_lanczos_3.webp 1200w" src=/issues/data-code/kowloon-grid_hu0ba43ba588d0d4b529a9453da4d2baae_307717_f0b163cf01b945a99c59287592d74ecc.webp width=760 height=176 loading=lazy data-zoomable></div></div></figure></p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Short description:</td><td>A toolkit to examine the quality of street view imagery</td></tr><tr><td>Lead developer:</td><td>yujun</td></tr><tr><td>Further reading:</td><td>Please read the <a href=https://doi.org/10.1016/j.jag.2022.103094 target=_blank rel=noopener>paper</a> published in JAG</td></tr><tr><td>Code:</td><td><a href=https://github.com/ualsg/SVI-Quality-Checker target=_blank rel=noopener><i class="fab fa-github"></i> Github repo</a></td></tr><tr><td>Main data source(s):</td><td>Mapillary</td></tr><tr><td>Citation:</td><td><details class=spoiler id=spoiler-17><summary>Click to view the BibTeX entry</summary><p>@article{2022_jag_svi_quality,
year = {2022},
title = {{A comprehensive framework for evaluating the quality of street view imagery}},
author = {Hou, Yujun and Biljecki, Filip},
journal = {International Journal of Applied Earth Observation and Geoinformation},
doi = {10.1016/j.jag.2022.103094},
pages = {103094},
volume = {115}
}</p></details></td></tr></tbody></table><h2 id=visual-soundscapes>Visual soundscapes</h2><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt srcset="/issues/data-code/visual-soundcapes_hu902ccdd4b042cfa0e1d82f631d4a4dbd_679125_77a6eb51a10162a84fe96184cc15cecc.webp 400w,
/issues/data-code/visual-soundcapes_hu902ccdd4b042cfa0e1d82f631d4a4dbd_679125_7549e794b21bbf0f5664928f3f796547.webp 760w,
/issues/data-code/visual-soundcapes_hu902ccdd4b042cfa0e1d82f631d4a4dbd_679125_1200x1200_fit_q80_h2_lanczos_3.webp 1200w" src=/issues/data-code/visual-soundcapes_hu902ccdd4b042cfa0e1d82f631d4a4dbd_679125_77a6eb51a10162a84fe96184cc15cecc.webp width=760 height=158 loading=lazy data-zoomable></div></div></figure></p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Short description:</td><td>Predicting the urban soundscape from street view imagery</td></tr><tr><td>Lead developer:</td><td>tianhong</td></tr><tr><td>Further reading:</td><td>Please read the <a href=https://doi.org/10.1016/j.compenvurbsys.2022.101915 target=_blank rel=noopener>paper</a> published in CEUS</td></tr><tr><td>Code:</td><td><a href=https://github.com/ualsg/Visual-soundscapes target=_blank rel=noopener><i class="fab fa-github"></i> Github repo</a></td></tr><tr><td>Citation:</td><td><details class=spoiler id=spoiler-19><summary>Click to view the BibTeX entry</summary><p>@article{2023_ceus_soundscapes,
author = {Zhao, Tianhong and Liang, Xiucheng and Tu, Wei and Huang, Zhengdong and Biljecki, Filip},
doi = {10.1016/j.compenvurbsys.2022.101915},
journal = {Computers, Environment and Urban Systems},
pages = {101915},
title = {Sensing urban soundscapes from street view imagery},
volume = {99},
year = {2023}
}</p></details></td></tr></tbody></table><h2 id=semantic-riverscapes>Semantic Riverscapes</h2><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt="A section of the Grand Canal in Tianjin" srcset="/issues/data-code/semantic-riverscapes_hudca020ee9bd61e44c185ae8c9cc6750a_85479_18148936d68230d9c0d25c038bce5306.webp 400w,
/issues/data-code/semantic-riverscapes_hudca020ee9bd61e44c185ae8c9cc6750a_85479_55679a5e6dd6ee5d9c009f84c049d860.webp 760w,
/issues/data-code/semantic-riverscapes_hudca020ee9bd61e44c185ae8c9cc6750a_85479_1200x1200_fit_q80_h2_lanczos.webp 1200w" src=/issues/data-code/semantic-riverscapes_hudca020ee9bd61e44c185ae8c9cc6750a_85479_18148936d68230d9c0d25c038bce5306.webp width=760 height=154 loading=lazy data-zoomable></div></div></figure></p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Short description:</td><td>A semantically annotated UAV oblique image dataset covering an urban river landscape</td></tr><tr><td>Lead developer:</td><td>junjie</td></tr><tr><td>Further reading:</td><td>Please read the <a href=https://doi.org/10.1016/j.landurbplan.2022.104569 target=_blank rel=noopener>paper</a> published in Landscape and Urban Planning</td></tr><tr><td>Download:</td><td><a href=https://github.com/ualsg/semantic-riverscapes-dataset target=_blank rel=noopener><i class="fab fa-github"></i> Github repo</a></td></tr><tr><td>Main data source(s):</td><td>Own data collection (UAV) and manual labelling</td></tr><tr><td>Coverage:</td><td>Tianjin (China)</td></tr><tr><td>Citation:</td><td><details class=spoiler id=spoiler-21><summary>Click to view the BibTeX entry</summary><p>@article{2022_land_semantic_riverscapes,
year = {2022},
title = {{Semantic Riverscapes: Perception and evaluation of linear landscapes from oblique imagery using computer vision}},
author = {Luo, Junjie and Zhao, Tianhong, and Cao, Lei and Biljecki, Filip},
journal = {Landscape and Urban Planning},
doi = {10.1016/j.landurbplan.2022.104569},
pages = {104569},
volume = {228}
}</p></details></td></tr></tbody></table><h2 id=classification-of-urban-morphology-with-deep-learning>Classification of Urban Morphology with Deep Learning</h2><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt srcset="/issues/data-code/urban-morphology-classification_hu5d4d4af13a53f9dee6df2227e73445ae_127664_e5a22bfb914d4307d49df3c60a65514e.webp 400w,
/issues/data-code/urban-morphology-classification_hu5d4d4af13a53f9dee6df2227e73445ae_127664_3761625f3c99c0f031cf224738709751.webp 760w,
/issues/data-code/urban-morphology-classification_hu5d4d4af13a53f9dee6df2227e73445ae_127664_1200x1200_fit_q80_h2_lanczos.webp 1200w" src=/issues/data-code/urban-morphology-classification_hu5d4d4af13a53f9dee6df2227e73445ae_127664_e5a22bfb914d4307d49df3c60a65514e.webp width=760 height=176 loading=lazy data-zoomable></div></div></figure></p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Short description:</td><td>Software to generate diagrams of the urban form at the city-scale and classify them using deep learning</td></tr><tr><td>Lead developer:</td><td>wangyang</td></tr><tr><td>Further reading:</td><td>Please read more in the <a href=https://doi.org/10.1016/j.compenvurbsys.2021.101706 target=_blank rel=noopener>paper</a> published in Computers, Environment and Urban Systems</td></tr><tr><td>Code:</td><td><a href=https://github.com/ualsg/Road-Network-Classification target=_blank rel=noopener><i class="fab fa-github"></i> Github repo</a></td></tr><tr><td>Citation:</td><td><details class=spoiler id=spoiler-23><summary>Click to view the BibTeX entry</summary><p>@article{2021_ceus_dl_morphology,
author = {Wangyang Chen and Abraham Noah Wu and Filip Biljecki},
doi = {10.1016/j.compenvurbsys.2021.101706},
journal = {Computers, Environment and Urban Systems},
pages = {101706},
title = {Classification of Urban Morphology with Deep Learning: Application on Urban Vitality},
url = {https://doi.org/10.1016/j.compenvurbsys.2021.101706},
volume = {90},
year = 2021
}</p></details></td></tr></tbody></table><h2 id=ganmapper-geographical-data-translation>GANmapper: geographical data translation</h2><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt srcset="/issues/data-code/ganmapper_hu87a483293dc3e9308ccc932f6a37db54_454621_9acd4dfb6e47d2795586e7a2e0b19a42.webp 400w,
/issues/data-code/ganmapper_hu87a483293dc3e9308ccc932f6a37db54_454621_ab589a1a3c54d752ba367be34d7c19fc.webp 760w,
/issues/data-code/ganmapper_hu87a483293dc3e9308ccc932f6a37db54_454621_1200x1200_fit_q80_h2_lanczos.webp 1200w" src=/issues/data-code/ganmapper_hu87a483293dc3e9308ccc932f6a37db54_454621_9acd4dfb6e47d2795586e7a2e0b19a42.webp width=760 height=246 loading=lazy data-zoomable></div></div></figure></p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Short description:</td><td>A building footprint generator using Generative Adversarial Networks from sparse data such as street networks</td></tr><tr><td>Lead developer:</td><td>abraham</td></tr><tr><td>Further reading:</td><td>Please read more in the <a href=https://doi.org/10.1080/13658816.2022.2041643 target=_blank rel=noopener>paper</a> published in the International Journal of Geographical Information Science</td></tr><tr><td>Code:</td><td><a href=https://github.com/ualsg/GANmapper target=_blank rel=noopener><i class="fab fa-github"></i> Github repo</a></td></tr><tr><td>Citation:</td><td><details class=spoiler id=spoiler-25><summary>Click to view the BibTeX entry</summary><p>@article{2022_ijgis_ganmapper,
year = {2022},
author = {Wu, Abraham Noah and Biljecki, Filip},
title = {{GANmapper: geographical data translation}},
journal = {International Journal of Geographical Information Science},
doi = {10.1080/13658816.2022.2041643},
volume = {36},
issue = {7},
pages = {1394-1422}
}</p></details></td></tr></tbody></table><h2 id=3d-dataset-of-all-public-housing-hdb-buildings-in-singapore>3D dataset of all public housing (HDB) buildings in Singapore</h2><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt="HDB 3D city model, Singapore" srcset="/issues/data-code/hdb3d-c4_att_cut_hufa858f7d9d12a7774cafb403efdb0df7_394567_375e442c75d33bd47031b90ffc787b31.webp 400w,
/issues/data-code/hdb3d-c4_att_cut_hufa858f7d9d12a7774cafb403efdb0df7_394567_b6461be75ba9e888efb038f98d8832f2.webp 760w,
/issues/data-code/hdb3d-c4_att_cut_hufa858f7d9d12a7774cafb403efdb0df7_394567_1200x1200_fit_q80_h2_lanczos_3.webp 1200w" src=/issues/data-code/hdb3d-c4_att_cut_hufa858f7d9d12a7774cafb403efdb0df7_394567_375e442c75d33bd47031b90ffc787b31.webp width=760 height=197 loading=lazy data-zoomable></div></div></figure></p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Short description:</td><td>About 12k semantically rich 3D buildings in Singapore in CityJSON and OBJ</td></tr><tr><td>Lead developer:</td><td>filip</td></tr><tr><td>Formats</td><td><a href=https://cityjson.org target=_blank rel=noopener>CityJSON</a>, <a href=https://en.wikipedia.org/wiki/Wavefront_.obj_file target=_blank rel=noopener>OBJ</a></td></tr><tr><td>Further reading:</td><td>Please read more here in our [blog post] or in the <a href=/publication/2020-3-dgeoinfo-3-d-asean/>paper</a> published at 3D GeoInfo</td></tr><tr><td>Download:</td><td><a href=https://github.com/ualsg/hdb3d-data target=_blank rel=noopener><i class="fab fa-github"></i> Github repo</a></td></tr><tr><td>Code:</td><td>The code used to generate the dataset is available in a <a href=https://github.com/ualsg/hdb3d-code target=_blank rel=noopener>separate <i class="fab fa-github"></i> Github repo</a></td></tr><tr><td>Main data source(s):</td><td>HDB, OpenStreetMap, OneMap. All open data</td></tr><tr><td>Coverage:</td><td>Singapore</td></tr><tr><td>Citation:</td><td><details class=spoiler id=spoiler-27><summary>Click to view the BibTeX entry</summary><p>@article{2020_3dgeoinfo_3d_asean,
author = {Biljecki, F.},
doi = {10.5194/isprs-annals-vi-4-w1-2020-37-2020},
journal = {ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences},
pages = {37&ndash;44},
title = {Exploration of open data in Southeast Asia to generate 3D building models},
volume = {VI-4/W1-2020},
year = {2020}
}</p></details></td></tr></tbody></table><h1 id=with-our-friendly-collaborators>With our friendly collaborators</h1><h2 id=longitudinal-thermographic-dataset-in-singapore>Longitudinal thermographic dataset in Singapore</h2><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt srcset="/issues/data-code/iris_hu95e1d248ccd3c0bffb5afff84c7f6e5e_2069950_9567a6acf916dbc47df44ee96b4d0087.webp 400w,
/issues/data-code/iris_hu95e1d248ccd3c0bffb5afff84c7f6e5e_2069950_e904c59228de7f57af0ae4f96bccc5e4.webp 760w,
/issues/data-code/iris_hu95e1d248ccd3c0bffb5afff84c7f6e5e_2069950_1200x1200_fit_q80_h2_lanczos_3.webp 1200w" src=/issues/data-code/iris_hu95e1d248ccd3c0bffb5afff84c7f6e5e_2069950_9567a6acf916dbc47df44ee96b4d0087.webp width=760 height=373 loading=lazy data-zoomable></div></div></figure></p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Short description:</td><td>More than a million thermographic images collected in our campus from ground-based thermal cameras over a long time, allowing users to determine the temperature trend of individual features such as buildings, roads, and vegetation in a tropical environment.</td></tr><tr><td>Lead developer:</td><td><a href=https://sg.linkedin.com/in/subin-lin-81710b211 target=_blank rel=noopener>Subin Lin</a> from NUS and the Berkeley Education Alliance for Research in Singapore (BEARS)</td></tr><tr><td>Data:</td><td><a href=https://github.com/buds-lab/project-iris-dataset target=_blank rel=noopener>Github</a></td></tr><tr><td>Further reading:</td><td>Please read the <a href=https://doi.org/10.1038/s41597-023-02749-0 target=_blank rel=noopener>paper</a> published in Scientific Data</td></tr><tr><td>Citation:</td><td><details class=spoiler id=spoiler-28><summary>Click to view the BibTeX entry</summary><p>@article{2023_sd_iris,
author = {Lin, Subin and Ramani, Vasantha and Martin, Miguel and Arjunan, Pandarasamy and Chong, Adrian and Biljecki, Filip and Ignatius, Marcel and Poolla, Kameshwar and Miller, Clayton},
doi = {10.1038/s41597-023-02749-0},
journal = {Scientific Data},
pages = {859},
title = {District-scale surface temperatures generated from high-resolution longitudinal thermal infrared images},
volume = {10},
year = {2023}
}</p></details></td></tr></tbody></table><h2 id=european-building-stock-characteristics-in-a-common-and-open-database>EUropean BUilding stock Characteristics in a Common and Open database</h2><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt srcset="/issues/data-code/eubucco_hu9f952710aca941b8c238e53a5fe99f0c_547189_6af9376de7573322f99c58de75cc2e0f.webp 400w,
/issues/data-code/eubucco_hu9f952710aca941b8c238e53a5fe99f0c_547189_a63c5efac538dc3772c92cd766ec7494.webp 760w,
/issues/data-code/eubucco_hu9f952710aca941b8c238e53a5fe99f0c_547189_1200x1200_fit_q80_h2_lanczos_3.webp 1200w" src=/issues/data-code/eubucco_hu9f952710aca941b8c238e53a5fe99f0c_547189_6af9376de7573322f99c58de75cc2e0f.webp width=760 height=209 loading=lazy data-zoomable></div></div></figure></p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Short description:</td><td>EUBUCCO is a scientific database of individual building footprints for 206 million buildings across the 27 European Union countries and Switzerland, together with three main attributes &ndash; building type, height and construction year &ndash; included for respectively 45%, 74%, 24% of the buildings.</td></tr><tr><td>Lead developer:</td><td><a href=https://milojevicdupontnikola.github.io target=_blank rel=noopener>Nikola Milojevic-Dupont</a> and <a href=https://www.susturbecon.tu-berlin.de/team/felix_wagner/ target=_blank rel=noopener>Felix Wagner</a>, <a href=https://www.mcc-berlin.net/ target=_blank rel=noopener>Mercator Research Institute for Global Commons and Climate Change</a> and TU Berlin</td></tr><tr><td>Website:</td><td><a href=https://eubucco.com target=_blank rel=noopener><i class="fas fa-home"></i> Website</a></td></tr><tr><td>Code:</td><td><a href=https://github.com/ai4up/eubucco target=_blank rel=noopener><i class="fab fa-github"></i> Github repo</a></td></tr><tr><td>Data:</td><td><a href=https://zenodo.org/record/6524781 target=_blank rel=noopener>Zenodo</a></td></tr><tr><td>Further reading:</td><td>Please read the <a href=https://doi.org/10.1038/s41597-023-02040-2 target=_blank rel=noopener>paper</a> published in Scientific Data</td></tr><tr><td>Citation:</td><td><details class=spoiler id=spoiler-29><summary>Click to view the BibTeX entry</summary><p>@article{2023_sd_eubucco,
author = {Milojevic-Dupont, Nikola and Wagner, Felix and Nachtigall, Florian and Hu, Jiawei and Br{"u}ser, Geza Boi and Zumwald, Marius and Biljecki, Filip and Heeren, Niko and Kaack, Lynn H. and Pichler, Peter-Paul and Creutzig, Felix},
doi = {10.1038/s41597-023-02040-2},
journal = {Scientific Data},
number = {1},
pages = {147},
title = {EUBUCCO v0.1: European building stock characteristics in a common and open database for 200+ million individual buildings},
volume = {10},
year = {2023}
}</p></details></td></tr></tbody></table><h2 id=3d-building-metrics-for-urban-morphology>3D building metrics for urban morphology</h2><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt srcset="/issues/data-code/3dbm_hu5cea8d7ccc7433e46e9c45f174373a08_120265_dab64d94139708241c2f195ef5ed7960.webp 400w,
/issues/data-code/3dbm_hu5cea8d7ccc7433e46e9c45f174373a08_120265_9a84e2f1e9e588bc8c3a34e2c5984eac.webp 760w,
/issues/data-code/3dbm_hu5cea8d7ccc7433e46e9c45f174373a08_120265_1200x1200_fit_q80_h2_lanczos_3.webp 1200w" src=/issues/data-code/3dbm_hu5cea8d7ccc7433e46e9c45f174373a08_120265_dab64d94139708241c2f195ef5ed7960.webp width=760 height=190 loading=lazy data-zoomable></div></div></figure></p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Short description:</td><td>3D Building Metrics. Elevating geometric analysis for urban morphology, solar potential, CFD etc to the next level</td></tr><tr><td>Lead developers:</td><td><a href=http://3d.bk.tudelft.nl/alabetski target=_blank rel=noopener>Anna Labetski</a> and <a href=http://3d.bk.tudelft.nl/svitalis target=_blank rel=noopener>Stelios Vitalis</a>, <a href=https://3d.bk.tudelft.nl target=_blank rel=noopener>3D Geoinformation</a>, TU Delft</td></tr><tr><td>Code:</td><td><a href=https://github.com/tudelft3d/3d-building-metrics target=_blank rel=noopener><i class="fab fa-github"></i> Github repo</a></td></tr><tr><td>Data:</td><td><a href=https://doi.org/10.7910/DVN/6QCRRF target=_blank rel=noopener>Repository</a></td></tr><tr><td>Further reading:</td><td>Please read the <a href=https://doi.org/10.1080/13658816.2022.2103818 target=_blank rel=noopener>paper</a> published in the International Journal of Geographical Information Science</td></tr><tr><td>Coverage:</td><td>Major cities in the Netherlands, extensible thanks to the code released open-source</td></tr><tr><td>Citation:</td><td><details class=spoiler id=spoiler-30><summary>Click to view the BibTeX entry</summary><p>@article{2023_ijgis_3dbm,
author = {Labetski, Anna and Vitalis, Stelios and Biljecki, Filip and Arroyo Ohori, Ken and Stoter, Jantien},
doi = {10.1080/13658816.2022.2103818},
journal = {International Journal of Geographical Information Science},
title = {3D building metrics for urban morphology},
year = {2023},
volume = {37},
issue = {1},
pages = {36-67}
}</p></details></td></tr></tbody></table><h2 id=3dfier-automatic-reconstruction-of-3d-city-models>3dfier: automatic reconstruction of 3D city models</h2><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt srcset="/issues/data-code/3dfier_hu8ee1fbab39e3dc5ba9f831fb651ffaf8_48564_cda574768bd4827640668a737b2d1a5b.webp 400w,
/issues/data-code/3dfier_hu8ee1fbab39e3dc5ba9f831fb651ffaf8_48564_eb3c7381d3e5822fda5e9f565c07adea.webp 760w,
/issues/data-code/3dfier_hu8ee1fbab39e3dc5ba9f831fb651ffaf8_48564_1200x1200_fit_q80_h2_lanczos.webp 1200w" src=/issues/data-code/3dfier_hu8ee1fbab39e3dc5ba9f831fb651ffaf8_48564_cda574768bd4827640668a737b2d1a5b.webp width=720 height=260 loading=lazy data-zoomable></div></div></figure></p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Short description:</td><td>Takes 2D GIS datasets (e.g. topographical datasets) and &ldquo;3dfies&rdquo; them (as in &ldquo;making them three-dimensional&rdquo;) by lifting every polygon to 3D</td></tr><tr><td>Lead developer:</td><td><a href=https://3d.bk.tudelft.nl target=_blank rel=noopener>3D Geoinformation</a>, TU Delft</td></tr><tr><td>Code:</td><td><a href=https://github.com/tudelft3d/3dfier target=_blank rel=noopener><i class="fab fa-github"></i> Github repo</a></td></tr><tr><td>Further reading:</td><td>Please read more in the <a href=https://doi.org/10.21105/joss.02866 target=_blank rel=noopener>paper</a> published in the Journal of Open Source Software</td></tr><tr><td>Citation:</td><td><details class=spoiler id=spoiler-31><summary>Click to view the BibTeX entry</summary><p>@article{2021_joss_3dfier,
author = {Ledoux, Hugo and Biljecki, Filip and Dukai, Balázs and Kumar, Kavisha and Peters, Ravi and Stoter, Jantien and Commandeur, Tom},
doi = {10.21105/joss.02866},
journal = {Journal of Open Source Software},
number = {57},
pages = {2866},
title = {3dfier: automatic reconstruction of 3D city models},
volume = {6},
year = {2021}
}</p></details></td></tr></tbody></table><h2 id=ifc2indoorgml>ifc2indoorgml</h2><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt srcset="/issues/data-code/ifc2indoorgml_hu7fe316859b88aff9209b4744e0810305_285973_88072470cddcf1db9de5492c4636d81a.webp 400w,
/issues/data-code/ifc2indoorgml_hu7fe316859b88aff9209b4744e0810305_285973_82f6258fb8ef0d7670b3cae76a403383.webp 760w,
/issues/data-code/ifc2indoorgml_hu7fe316859b88aff9209b4744e0810305_285973_1200x1200_fit_q80_h2_lanczos_3.webp 1200w" src=/issues/data-code/ifc2indoorgml_hu7fe316859b88aff9209b4744e0810305_285973_88072470cddcf1db9de5492c4636d81a.webp width=760 height=236 loading=lazy data-zoomable></div></div></figure></p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Short description:</td><td>A tool allowing to generate IndoorGML files from IFC input models</td></tr><tr><td>Lead developer:</td><td><a href=https://www.unsw.edu.au/staff/abdoulaye-diakite target=_blank rel=noopener>Abdoulaye Diakite</a>, <a href=http://grid.unsw.edu.au/ target=_blank rel=noopener>GRID</a>, University of New South Wales</td></tr><tr><td>Code:</td><td><a href=https://github.com/grid-unsw/ifc2indoorgml target=_blank rel=noopener><i class="fab fa-github"></i> Github repo</a></td></tr><tr><td>Further reading:</td><td><a href=/publication/2022-isprs-ifc-2-indoorgml/>Paper</a></td></tr><tr><td>Citation:</td><td><details class=spoiler id=spoiler-32><summary>Click to view the BibTeX entry</summary><p>@article{2022_isprs_ifc2indoorgml,
author = {Diakite, AA and Díaz-Vilariño, L and Biljecki, F and Isikdag, Ü and Simmons, S and Li, K and Zlatanova, S},
doi = {10.5194/isprs-archives-xliii-b4-2022-295-2022},
journal = {Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci.},
pages = {295&ndash;301},
title = {ifc2indoorgml: An open-source tool for generating IndoorGML from IFC},
volume = {XLIII-B4-2022},
year = {2022}
}</p></details></td></tr></tbody></table><h2 id=aida---annotated-image-database-of-architecture>AIDA - Annotated Image Database of Architecture</h2><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Short description:</td><td>A repository of architectural photographs worldwide, labelled with a vast list of hierarchical categories and a series of auxiliary annotations</td></tr><tr><td>Lead developer:</td><td>Chen Jielin</td></tr><tr><td>Further reading:</td><td><a href=/publication/2021-caadria-aida/>Paper</a></td></tr><tr><td>Download:</td><td><a href=https://doi.org/10.7910/DVN/IGNELZ target=_blank rel=noopener>Harvard Dataverse</a></td></tr><tr><td>Main data source(s):</td><td>ArchDaily</td></tr><tr><td>Citation:</td><td><details class=spoiler id=spoiler-33><summary>Click to view the BibTeX entry</summary><p>@inproceedings{2021_caadria_aida,
author = {Chen, Jielin and Stouffs, Rudi and Biljecki, Filip},
booktitle = {Proceedings of the 26th International Conference of the Association for Computer-Aided Architectural Design Research in Asia (CAADRIA) 2021},
pages = {161&ndash;170},
title = {Hierarchical (Multi-Label) Architectural Image Recognition and Classification},
volume = {1},
year = {2021}
}</p></details></td></tr></tbody></table></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fissues-osu%2Fissues%2Fdata-code%2F&amp;text=Data+and+code" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Fissues-osu%2Fissues%2Fdata-code%2F&amp;t=Data+and+code" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Data%20and%20code&amp;body=https%3A%2F%2Fissues-osu%2Fissues%2Fdata-code%2F" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fissues-osu%2Fissues%2Fdata-code%2F&amp;title=Data+and+code" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Data+and+code%20https%3A%2F%2Fissues-osu%2Fissues%2Fdata-code%2F" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https%3A%2F%2Fissues-osu%2Fissues%2Fdata-code%2F&amp;title=Data+and+code" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2025 ISSUES - Investigatng Spatial Structures in Urban Environments.</p><p class=powered-by>Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target=_blank rel=noopener>Hugo Blox Builder</a> — the free, <a href=https://github.com/HugoBlox/hugo-blox-builder target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/issues/js/vendor-bundle.min.391d344a129df56f7ad674c2c2ed04e8.js></script><script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script><script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script><script id=page-data type=application/json>{"use_headroom":true}</script><script src=/issues/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script><script src=/issues/en/js/wowchemy.min.38d80f314799b3be4a3e21db07a87a78.js></script><script src=/issues/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/issues/js/wowchemy-publication.9c0e895144aef5a693008b5c5d450147.js type=module></script></body></html>